{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors, Correlations and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure you have watched the Chapter 4 video(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will learn the following things in this Chapter\n",
    "\n",
    "- How to combine (with weighting) measurements based on their errors.\n",
    "- Covariance and correlation.\n",
    "- Correlation statistics.\n",
    "- How to interpret a correlation statistic.\n",
    "- Classic hypothesis testing and confidence intervals.\n",
    "- How to interpret a confidence interval.\n",
    "- How to use Python programming to do the above.\n",
    "- After completing this notebook you will be able to attempt CA 1 questions 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the measurement of a particular quantity is subject to many, independent and random errors, then the *central limit theorem* allows us to use the normal distribution to model the quantity’s errors.\n",
    "\n",
    "\n",
    "Note that there are two types of errors:\n",
    "- **statistical errors** - from random nature of measurement process, can be reduced by increasing the number of measurements and averaging over them.\n",
    "- **systematic errors** - these arise from flawed measurements (eg a rogue voltmeter adding +2V to every measurement because its not properly calibrated). This is easy to spot as it remains even after repeating measurements multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining measurements with different errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two students, let's call them $A$ and $B$, who make a measurement of the length of our snake, $x$. Student $A$ finds the length to be $x = x_A \\pm \\sigma_A$, while student $B$ finds that $x = x_A \\pm \\sigma_A$. Given that both sets of data are valid estimates of the snake's length, we'd like to combine the results from the two experiments, to get a new, and hopefully improved result $x_{AB}$, with an associated uncertainty $\\sigma_{AB}$.\n",
    "\n",
    "How to proceed? It is tempting to simply average the two results, e.g. $x_{AB} = \\dfrac{x_A + x_B}{2}$, but this feels a bit fishy if the two uncertainties $\\sigma_A$ and $\\sigma_B$ are not equal. Why should they have equal weighting, if one is less accurate (higher uncertainty) than the other? The answer is to weight the values according to their uncertainties, to produce a **weighted average**.\n",
    "\n",
    "$\\hat{x_0} = \\dfrac{w_A x_A + w_B x_B} {w_A + w_B}$\n",
    "\n",
    "where $\\hat{x_0}$ denotes the weighted average, and $\\hat{\\sigma}_{x_0}$ is the standard deviation\n",
    "\n",
    "$\\hat{\\sigma}_{x_0} = \\dfrac{1}{\\sqrt{\\sum w_i}}.$\n",
    "\n",
    "$w_i$ denotes the individual weights of each component in the average and $w_A = 1/\\sigma_A^2$ and $w_B = 1/\\sigma_B^2$.  \n",
    "\n",
    "This type of weighting - also called optimal weighting - is extremely important in data analysis.  Optimal weighting allows you to take account of all data points, with each point contributing to the final result in a way than depends on how well you trust the data (i.e. the variance of the point). The problem is, that you need to know something about the error in each point (not always the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to assume once again that the errors in our measurements are normally distributed, and the two experiments performed by students $A$ and $B$ were completely independent. In that case, the probability that the students would obtain their results is given by,\n",
    "\n",
    "$P_{x_0}(x_A) \\propto \\dfrac{1}{\\sigma_A} e^{-\\dfrac{(x_A -x_0)^2}{ 2\\sigma_A}}$\n",
    "\n",
    "for student $A$ and\n",
    "\n",
    "$P_{x_0}(x_B) \\propto \\dfrac{1}{\\sigma_B} e^{-\\dfrac{(x_B -x_0)^2 }{2\\sigma_B}}$\n",
    "\n",
    "for student $B$. Note that the probabilities depend on the unknown, but true value of the measurement $x_0$.\n",
    "\n",
    "So the probability that **both** students found the lengths $x_A$ and $x_B$ is then simply,\n",
    "\n",
    "$P_{x_0}(x_A \\cap x_B) = P_{x_0}(x_A , x_B)$\n",
    "\n",
    "$= P_{x_0}(x_A) \\times P_{x_0}(x_B) \\propto \\dfrac{1}{\\sigma_A \\sigma_B} e^{-\\chi^2/2}$,\n",
    "\n",
    "\n",
    "where we have introduced the notation $\\chi^2$ (chi-squared) as a shorthand for,\n",
    "\n",
    "$\\chi^2 = \\left( \\dfrac{x_A - x_0}{\\sigma_A} \\right)^2 + \\left( \\dfrac{x_B- x_0}{\\sigma_B} \\right)^2$.\n",
    "\n",
    "Using the principle of maximum likelihood from Chapter 3, we can see that $P_{x_0}(x_A , x_B)$ has a maximum when $\\chi^2$ has a minimum. So we want to know the value of $x_0$ that would maximise the chances of $A$ finding $x_A$ {\\bf and} $B$ finding $x_B$. To do this, we need to differentiate $\\chi^2$ and set the derivative equal to zero,\n",
    "\n",
    "$2 \\dfrac{x_A - x_0}{\\sigma_A} + 2 \\dfrac{x_B- x_0}{\\sigma_B} = 0$\n",
    "\n",
    "The solution for $x_0$ is then simply,\n",
    "\n",
    "$\\mbox{best estimate for~} x_0 = \\left( \\dfrac{x_A}{\\sigma_A^2} + \\dfrac{x_B}{\\sigma_B^2}  \\right) \\Big/ \\left( \\dfrac{1}{\\sigma_A^2} + \\dfrac{1}{\\sigma_B^2}  \\right)$\n",
    "\n",
    "If we define weights to have the form $w_A = \\dfrac{1}{\\sigma_A}^2$ and $w_B = \\dfrac{1}{\\sigma_B}^2$, then we can tidy this up to obtain,\n",
    "\n",
    "$\\hat{x_0} = \\dfrac{w_A x_A + w_B x_B} {w_A + w_B}$\n",
    "\n",
    "where $\\hat{x_0}$ denotes the weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During lab experiments, you will have learnt how to identify sources of error in your experiments, and how to propagate these errors through to the final result. \n",
    "\n",
    "We will briefly discuss the maths underlying the error propagation here, since it provides the background for an important property in statistics: the covariance. Let's first assume that we have a function $f$ that is dependent on some measured quantity $x$, and yields a value $y$ that we are interested in knowing, such that $y = f(x)$. Now the measurements of $x$ are associated with some random error, $\\sigma_x$, and so the final value of $y$ will also have an error $\\sigma_y$. How do we calculate $\\sigma_y$?\n",
    "\n",
    "Assuming the errors in $x$ are small, and are close to the true value $\\hat{x}$, we can expand $f(x)$ around the point $\\hat x$ and derive an expression for $y - \\hat y$ which leads to:\n",
    "\n",
    "$\\sigma_y^2 = \\left( \\dfrac{df} {dx} \\right)^2_{\\hat x} \\, \\sigma_x^2$,\n",
    "\n",
    "For two variables eg $z = f(x,y)$, we get the following: \n",
    "\n",
    "$\\sigma_z^2 = \\left( \\dfrac{\\partial f} {\\partial x} \\right)^2 \\sigma_x^2 + \\left( \\dfrac{\\partial f} {\\partial y} \\right)^2 \\sigma_y^2 + 2 \\dfrac{\\partial f} {\\partial x} \\dfrac{\\partial f} {\\partial y} \\sigma_{xy}.$\n",
    "\n",
    "Ignoring the last term on the RHS for a moment, we see that the expression is the normal error propagation formula that you may have seen during your lab work (for independent errors). If  $\\sigma_x$ and $\\sigma_y$ are not independent, then we need the last term! This is called the *covariance*.\n",
    "\n",
    "$\\sigma_{xy} = \\dfrac{1}{N}\\sum (x - \\hat x) (y - \\hat y).$\n",
    "\n",
    "In fact, this is called the population covariance.\n",
    "\n",
    "The variance of a variable describes how much the values are spread. The covariance instead is a measure that tells the amount of dependency between two variables. A positive covariance means that the values of the first variable are large when the values of the second variables are also large. A negative covariance means the opposite: large values from one variable are associated with small values of the other. For truly independent variables $\\sigma_{xy}$ will be zero.\n",
    "\n",
    "We know from Chapter 3 that our experiments are *sampling* from the underlying true population, so we need to define a sample covariance:\n",
    "\n",
    "$\\sigma_{xy} = \\dfrac{1}{N-1}\\sum (x - \\hat x) (y - \\hat y).$\n",
    "\n",
    "Sometimes it can be difficult to conclude where two sources of error (or two parameters) are indeed correlated – often this is the case when the number of data points is small. So what then? If we assume the errors are independent, can we find a way to get the upper limit on the error, to ensure that we don’t miss anything?\n",
    "\n",
    "Yes, we can use the **Schwarz inequality** to write $|\\sigma_{xy}| \\le \\sigma_x \\sigma_y$ and therefore\n",
    "\n",
    "$\\sigma_z \\le |\\dfrac{\\partial f}{\\partial x}|\\sigma_x + |\\dfrac{\\partial f}{\\partial y}|\\sigma_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance keeps the scale of the variables $x$ and $y$, and therefore can take on any value. This makes interpretation difficult and comparing covariances to each other impossible. For example, $\\sigma_{XY}  = 5.2$ and $\\sigma_{ZQ}= 3.1$ tell us that these pairs are positively associated, but it is difficult to tell whether the relationship between $X$ and $Y$ is stronger than $Z$ and $Q$ without looking at the means and distributions of these variables.  We can normalise the covariance to give us both direction and strength of the correlation between these parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first assume that we have a function $f$ that is dependent on some measured quantity $x$, and yields a value $y$ that we are interested in knowing, such that $y = f(x)$. Now the measurements of $x$ are associated with some random error, $\\sigma_x$, and so the final value of $y$ will also have an error $\\sigma_y$. How do we calculate $\\sigma_y$?\n",
    "\n",
    "Assuming the errors in $x$ are small, and are close to the true value $\\hat{x}$, we can expand $f(x)$ around the point $\\hat x$,\n",
    "\n",
    "$f(x) = f(\\hat x) + (x - \\hat x) \\left( \\dfrac{df} {dx} \\right)_{\\hat x}  + \\dotsb$\n",
    "\n",
    "If we now identify $\\hat y = f(\\hat x)$, then we can see that,\n",
    "\n",
    "$y - \\hat y = f(x) -  f(\\hat x) \\approx  (x - \\hat x) \\left( \\dfrac{df} {dx} \\right)_{\\hat x}.$\n",
    "\n",
    "which gives us an expression for how the value of $y$ derived from our measured value of $x$, relates to the true values of both $y$ and $x$, which are given by $\\hat y$ and $\\hat x$. If we then take many measurements of $x$, we can use the expression above to write the standard deviation about the mean, as\n",
    "\n",
    "$\\dfrac {1}{N}\\sum_i^N (y_i - \\hat y)^2 = \\left( \\dfrac{df} {dx} \\right)^2_{\\hat x} \\dfrac {1}{N}\\sum_i^N (x_i - \\hat x)^2$\n",
    "\n",
    "or simply,\n",
    "\n",
    "$\\sigma_y^2 = \\left( \\dfrac{df} {dx} \\right)^2_{\\hat x} \\, \\sigma_x^2$.\n",
    "\n",
    "For two variables $x$ and $y$ where $z = f(x,y)$, we expand our function $f$ around the true values of $\\hat x$ and $\\hat y$ using Taylor's expansion, to get,\n",
    "\n",
    "$\\hat{z} = ~\\hat{f}(x, y) =  f(\\hat x, \\hat y) + \\left( \\dfrac{\\partial f} {\\partial x} \\right)_{\\hat x} (x - \\hat x) + \\left( \\dfrac{\\partial f} {\\partial y} \\right)_{\\hat y} (y - \\hat y) + \\dotsb$\n",
    "\n",
    "Now write the bracket term in the variance for z:\n",
    " \n",
    "$(z - \\hat z)^2 = ~( f(x, y) - f(\\hat x, \\hat y) )^2$\n",
    "\n",
    "$ (z - \\hat z)^ \\approx \\left( \\dfrac{\\partial f} {\\partial x} \\right)^2 (x - \\hat x)^2 + \\left( \\dfrac{\\partial f} {\\partial y} \\right)^2 (y - \\hat y)^2 + 2 \\dfrac{\\partial f} {\\partial x} \\dfrac{\\partial f} {\\partial y}  (x - \\hat x) (y - \\hat y),\n",
    "$\n",
    "\n",
    "which then gives us the result that,\n",
    "\n",
    "$\\sigma_z^2 = \\left( \\dfrac{\\partial f} {\\partial x} \\right)^2 \\sigma_x^2 + \\left( \\dfrac{\\partial f} {\\partial y} \\right)^2 \\sigma_y^2 + 2 \\dfrac{\\partial f} {\\partial x} \\dfrac{\\partial f} {\\partial y} \\sigma_{xy}.$\n",
    "\n",
    "$\\sigma_z^2 = \\left( \\dfrac{\\partial f} {\\partial x} \\right)^2 \\sigma_x^2 + \\left( \\dfrac{\\partial f} {\\partial y} \\right)^2 \\sigma_y^2 + 2 \\dfrac{\\partial f} {\\partial x} \\dfrac{\\partial f} {\\partial y} \\sigma_{xy}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we measure the degree of correlation (the association between the observed values of two variables) in data?  In almost any business, or modelling that happens, it is useful to express one quantity in terms of its relationship with others. For example, sales might increase when the marketing department spends more on TV advertisements.  Often, correlation is the first step to understanding relationships between variables and subsequently building better business and statistical models.\n",
    "\n",
    "Two variables may have a positive association, so that as the values for one variable increase, so do the values of the other variable. Alternatively, the association could be negative or neutral. Correlation quantifies this association, often as a measure between the values -1 to 1 for perfectly negatively correlated and perfectly positively correlated. The calculated correlation is referred to as the “correlation coefficient.” This correlation coefficient can then be interpreted to describe the measures.\n",
    "\n",
    "For a linear function, the extent to which data points $(x_1, y_1)... (x_N, y_N)$ support a linear correlation is given by the *linear correlation coefficient* sometimes called the Pearson correlation coefficient,\n",
    "\n",
    "$r =  \\dfrac{\\sigma_{xy}} {\\sigma_x\\,\\sigma_y}$\n",
    "\n",
    "$ r = \\dfrac{\\sum(x - \\hat x)(y - \\hat y)} { \\sqrt{\\sum (x - \\hat x)^2 \\sum ( y- \\hat y)^2} }.\n",
    "$\n",
    "\n",
    "If $r$ is close to $\\pm 1$, then we would say that the points are correlated.  Completetly uncorrelated points would have $r=0$. An illustration of this is seen in the image below:\n",
    "\n",
    "<img src=\"https://github.com/haleygomez/Data-Analysis-2021/raw/master/blended_exercises/Chapter%204/correlation.png\" width=\"700\">\n",
    "\n",
    "So, why is correlation a useful metric?\n",
    "\n",
    "- Correlation can help in predicting one quantity from another\n",
    "- Correlation can (but often does not) indicate the presence of a causal relationship\n",
    "- Correlation is used as a basic quantity and foundation for many other modeling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does Pearson $r$ correlation equal to $\\pm 1$ give us the strength of the correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Click below to see the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Schwarz inequality $|\\sigma_{x,y}| \\le \\sigma_x\\sigma_y$.\n",
    "\n",
    "Now imagine that all the points do indeed lie exactly on a straight line $y = A+Bx$. Since $y_i = A + Bx_i$ and $\\hat y = A + B\\hat x$, then $y_i - \\hat y = B(x_i - \\hat x)$. Using this to remove the $y$s, we get,\n",
    "\n",
    "$r  = \\dfrac{B \\sum(x_i - \\hat x)^2} { \\sqrt{\\sum (x_i - \\hat x)^2 B^2 \\sum (x - \\hat x)^2} } = \\dfrac{B}{|B|} = \\pm 1.$\n",
    "\n",
    "However in the case that there's no correlation with $x$ and $y$, then although the numerator will fluctuate $+/-$ve, the dominator will always be positive and drive $r$ to zero as the number of points tend to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how close to 1 is close enough?  It turns out it is actually possible to work out the probability that $r$ will exceed a given value $r_0$ after a given number of uncorrelated data points are considered, i.e. $P_N(|r| \\geq r_0)$. \n",
    "\n",
    "If we look at standard probability tables (see table below), the probability of getting a correlation coefficient of $r \\ge 0.7$ is 51% for a sample of $N=3$ **even if 2 variables are uncorrelated**.  Therefore we need to combine any $r$ correlation value with some measure of the probability of getting that value just by random given the number of data points you have.  We will return to this in Chapter 5.\n",
    "\n",
    "<img src=\"https://github.com/haleygomez/Data-Analysis-2021/raw/master/blended_exercises/Chapter%204/ro.png\" width=\"700\">\n",
    "\n",
    "Note that the correlation between two variables that each have a Gaussian distribution can be calculated using standard methods such as the Pearson's correlation but this procedure cannot be used for data that does not have a Gaussian distribution. Instead, we will see something at the end of the Block which can allow us to look for correlations without assuming a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this snippet of code to generate some fake data which we will use to see if there is a correlation.\n",
    "\n",
    "`import numpy as np`\n",
    "\n",
    "`from numpy.random import randn`\n",
    "\n",
    "`data1 = 20 * randn(1000) + 100`\n",
    "\n",
    "`data2 = data1 + (10 *randn(1000) + 50)`\n",
    "\n",
    "Find the covariance of this dataset and the Pearson $r$ correlation and comment on what you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Click below to see the Solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of x is 100.18\n",
      "the mean of y is 149.84\n",
      "the covariance between x and y is 369.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from numpy.random import randn\n",
    "\n",
    "# the line below makes the plot appear in the jupyter notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "# let's generate some random data\n",
    "data1 = 20 * randn(1000) + 100\n",
    "data2 = data1 + (10 *randn(1000) + 50)\n",
    "n = len(data1)\n",
    "\n",
    "# define a function that estimates covariance\n",
    "def cov(x,y,n):\n",
    "    x_hat = np.mean(x)\n",
    "    y_hat = np.mean(y)\n",
    "    return np.sum((x-x_hat)*(y-y_hat))/(n-1)\n",
    "\n",
    "# let's work out mean of the data\n",
    "xhat = np.mean(data1)\n",
    "yhat = np.mean(data2)\n",
    "\n",
    "print('the mean of x is {:.2f}'.format(xhat))\n",
    "print('the mean of y is {:.2f}'.format(yhat))\n",
    "\n",
    "# covariance between the datasets\n",
    "covar = cov(data1,data2,n)\n",
    "print('the covariance between x and y is {:.2f}'.format(covar))\n",
    "\n",
    "# plot\n",
    "plt.scatter(data1, data2)\n",
    "plt.xlabel('data1')\n",
    "plt.ylabel('data2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks to be highly correlated. Now it's not too much more work to calculate the linear correlation coefficient $r$. Here we will see how to do this using the inbuilt python function from the `scip.stats` package. Many of the things we'll do in the course have inbuilt routines in python but part of the coursework will see you doing it from scratch to check understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation is: 0.886\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr, _ = pearsonr(data1, data2)\n",
    "print('Pearsons correlation is: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from our notes that this value of $r$ indicates that the data is strongly correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a hypothesis is one of the foundations of data analysis. Examples include: does this drug make people better? Is the die fair? Are an observed population of low-mass galaxies consistent with the predictions from $\\Lambda$CDM? Did CERN really detect the Higg's Boson? We'll start this section by outlining the formal ideas behind hypothesis testing, and then look at some classic examples.\n",
    "\n",
    "The most common form of hypothesis testing is  involves trying to find the unknown parameter $\\theta$ that is part of a model $f(\\theta)$. Now you might have a best guess for the unknown parameter, and an associated uncertainty, so really we're not always testing if $\\theta$ is an exact value, but more generally whether $\\theta \\in \\Theta$, that is $\\theta$ is part of some set of possible values $\\Theta$. From our best guess of $\\theta$, what we're trying to determine is whether $\\theta \\in \\Theta_0$ or $\\theta \\in \\Theta_1$, and where,\n",
    "\n",
    "$\\Theta_0 \\cup \\Theta_1 = \\Theta$ and $\\Theta_0 \\cap \\Theta_1 = 0$.\n",
    "\n",
    "We then make a set of new observations of some outcome of the model $X = \\{x_1, x_2, x_3, \\ldots\\}$, and we want to test whether they support the idea that, say, $\\theta \\in \\Theta_1$. We also know the probability of the model predicting the data, which is given by $p(X, \\theta)$.\n",
    "\n",
    "This is **Null Hypothesis Significance Testing**, which we will abbreviate as NHST.  Hypothesis testing is the bread and butter of inferential statistics and a critical skill in the repertoire of a data scientist. \n",
    "\n",
    "\\begin{align}\n",
    "H_0 &:&~\\theta \\in \\Theta_0 &~~ \\text{the null hypothesis} \\\\\n",
    "H_1 &:&~\\theta \\in \\Theta_1 &~~ \\text{the alternative hypothesis}\n",
    "\\end{align}\n",
    "\n",
    "The null hypothesis assumes that nothing interesting happens/happened. The alternative hypothesis is, where the action is i.e. some observation/ phenomenon is real (i.e. not a fluke) and statistical analysis will give us more insights on that.\n",
    "\n",
    "Statisticians take a pessimistic sort of view and start with the Null hypothesis ie the *null* hypothesis is what we are going to assume is true, thought we are normally trying to show that it is not!  We then compute a statistic and then ask \"What is the chance of observing the test-statistic for this sample (considering its size and the probability governing the system), purely randomly (ie if the Null hypothesis were true)?\"\n",
    "\n",
    "This chance — the probability value of observing the test-statistic — is the so-called $p-$value. The $p-$value is the probability of getting a value of the test statistic at least as extreme as that\n",
    "actually observed value purely by chance, if the null hypothesis is true.\n",
    "\n",
    "\n",
    "So the kind of statement you may read is something like \"Feeding chocolate to female chickens gives a proportion of male chicks that is significantly less than 50% (with p=0.015).\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to make two types of error in classical hypothesis testing when you reject a null hypothesis, and they have well defined names:\n",
    "\n",
    " - Type I error is when you (for some reason) reject $H_0$ when it is true (eg experimental results deviate from this purely by chance) - this results in a false positive.\n",
    " - Type II error is when you decide not to reject $H_0$ when it is false.\n",
    "\n",
    "We can assign a probability to our Type I error ie what is the significance level at which we reject the null hypothesis. We often choose a value of $\\alpha=0.05$ but it can be determined by the individual or team.   When a p-value is less than or equal to the significance level, you reject the null hypothesis. As an example, if a test gives the p-value, $p = 0.03$, the null hypothesis would be rejected at significance level $\\alpha = 0.05$, but not at the more conservative significance level $\\alpha = 0.01$.\n",
    "\n",
    "**For the typical error rate of $p=0.05$ or equivalently 5%, 1 experiment in every 20 will yield incorrect conclusions.**\n",
    "\n",
    "Remember, you cannot prove that something is correct in classical hypothesis testing, only prove that it is wrong. This is why the errors focus on $H_0$ - at best you can accept that $H_0$ is correct, and thus our hypothesis that  $\\theta \\in \\Theta_1$ is wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval contains a parameter (like a population mean) with a certain confidence level. In other words, it tells you the likely location for a population parameter. \n",
    "\n",
    "Often data analysts use a confidence interval of 95% to quote their numbers.  For example, your mean battery life might be 105 hours, and the  95% confidence interval in that number ranges from 100 to 110 hours. That means if you repeat your experiment millions of times, 95% of the time the average battery life will fall into that range and the other 5% it will not.\n",
    "\n",
    "Put it this way, of the many 95% confidence intervals produced from multiple experiments in order to try and measure a variable, 95% are expected to contain the true value. The other 5% of experiments may completely fail!   This is a bit awkward!\n",
    "\n",
    "\n",
    "A statement that you may see is ”Feeding chocolate to female chickens produced 36.1% male chicks with a 95% confidence interval of 25.9 to 47.4%.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox news discusses a publication where a 95% confidence interval for the average amount of television watched by Americans was found to be (2.69, 6.04) hours.  They state that this means that 95% of all Americans\n",
    "watch between 2.69 and 6.04 hours of television. Is this statement factual?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Click below to see the Solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement by Fox news is false. The correct statement would be that we are 95% confident that the average amount of television watched by Americans is between 2.69 and 6.04 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to tackle the **Chapter 4 quiz** on Learning Central and the [Chapter 4 yourturn notebook](https://github.com/haleygomez/Data-Analysis-2021/blob/master/blended_exercises/Chapter%204/Chapter4_yourturn.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
